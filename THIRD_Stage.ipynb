{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.16/00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colmsam/miniconda/envs/cern_env/lib/python3.7/site-packages/root_numpy/__init__.py:46: RuntimeWarning: numpy 1.16.4 is currently installed but you installed root_numpy against numpy 1.9.3. Please consider reinstalling root_numpy for this numpy version.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from root_pandas import read_root\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_curve, confusion_matrix, precision_score, recall_score\n",
    "import gc ; gc.enable()\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from fnmatch import filter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc ; gc.enable()\n",
    "from TOF_COM_calculation import TOF_COM_calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate COM variables for the TwoBody candidates df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [\n",
    "'runNumber'\n",
    ",'eventNumber'\n",
    ",'nCandidate']\n",
    "\n",
    "COM_TB_stuff = [\"TwoBody_M\",\n",
    "\"TwoBody_PE\",\n",
    "\"TwoBody_PX\",\n",
    "\"TwoBody_PY\",\n",
    "\"TwoBody_PZ\",\n",
    "\"TwoBody_ENDVERTEX_X\",\n",
    "\"TwoBody_ENDVERTEX_Y\",\n",
    "\"TwoBody_ENDVERTEX_Z\",\n",
    "\"TwoBody_OWNPV_X\",\n",
    "\"TwoBody_OWNPV_Y\",\n",
    "\"TwoBody_OWNPV_Z\",\n",
    "\"Track1_PX\",\n",
    "\"Track1_PY\",\n",
    "\"Track1_PZ\",\n",
    "\"Track2_PX\",\n",
    "\"Track2_PY\",\n",
    "\"Track2_PZ\",\n",
    "'Track1_ProbNNe',\n",
    "'Track1_ProbNNk',\n",
    "'Track1_ProbNNp',\n",
    "'Track1_ProbNNpi',\n",
    "'Track1_ProbNNmu',\n",
    "'Track1_ProbNNghost',\n",
    "'Track2_ProbNNe',\n",
    "'Track2_ProbNNk',\n",
    "'Track2_ProbNNp',\n",
    "'Track2_ProbNNpi',\n",
    "'Track2_ProbNNmu',\n",
    "'Track2_ProbNNghost']\n",
    "\n",
    "features = [\n",
    "'TwoBody_DIRA_OWNPV'\n",
    ",'TwoBody_DOCAMAX'\n",
    ",'TwoBody_ENDVERTEX_CHI2'\n",
    ",'noexpand:log(TwoBody_FDCHI2_OWNPV)'    \n",
    ",'TwoBody_FD_OWNPV'\n",
    ",'TwoBody_M'\n",
    ",'TwoBody_IPCHI2_OWNPV'\n",
    ",'TwoBody_Mcorr'\n",
    ",'TwoBody_PT'\n",
    ",'noexpand:log(Track1_MINIPCHI2)'\n",
    ",'Track1_PT'\n",
    ",'noexpand:log(Track2_MINIPCHI2)'\n",
    ",'Track2_PT',\n",
    "'Track1_Charge',\n",
    "'Track2_Charge']\n",
    "\n",
    "\n",
    "labels = [\n",
    "'SignalB_ID' ]\n",
    "\n",
    "TBcols = ids+ COM_TB_stuff + features + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_root('TaggingJpsiK2012_tiny_fix_fix.root', columns= TBcols )\n",
    "\n",
    "#as eventNumber is not unique, but for a given run number, it is, so we can combine the two to get a completely unique id for each event\n",
    "df.index = df.apply(lambda x:str(int(x.runNumber)) + str(int(x.eventNumber))+'-'+str(int(x.nCandidate)), axis=1 )\n",
    "promising_TB_ids = pd.read_csv('second_round_twobody_candidates_tiny.csv', header=None)[0]\n",
    "\n",
    "#cutting out extra track canidates from TwoBodyCandidates that weren't recoginised by first stage MVA\n",
    "TB_df = df.loc[promising_TB_ids, :]\n",
    "\n",
    "#get rid of other id like columns as now no need for them\n",
    "TB_df = TB_df.drop(columns=['runNumber', 'eventNumber', 'nCandidate'], axis=0)\n",
    "\n",
    "#calculate COM variables for TB_df\n",
    "COM_TB_df = TOF_COM_calculator(df = TB_df, PXs=[\"Track1_PX\", \"Track2_PX\"], PYs=[\"Track1_PY\", \"Track2_PY\"], PZs=[\"Track1_PZ\", \"Track2_PZ\"], proton_probs=['Track1_ProbNNp','Track2_ProbNNp'], kaon_probs=['Track1_ProbNNk','Track2_ProbNNk'], pion_probs=['Track1_ProbNNpi','Track2_ProbNNpi'], electron_probs=['Track1_ProbNNe','Track2_ProbNNe'], muon_probs=['Track1_ProbNNmu','Track2_ProbNNmu'], names=['track1', 'track2'] )\n",
    "\n",
    "del df, TB_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 521, -521])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COM_TB_df.SignalB_ID.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caclulate COM variables for the good extra tracks df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [\n",
    "'runNumber'\n",
    ",'eventNumber'\n",
    ",'nCandidate']\n",
    "\n",
    "COM_stuff = [\"TwoBody_M\",\n",
    "\"TwoBody_PE\",\n",
    "\"TwoBody_PX\",\n",
    "\"TwoBody_PY\",\n",
    "\"TwoBody_PZ\",\n",
    "\"TwoBody_ENDVERTEX_X\",\n",
    "\"TwoBody_ENDVERTEX_Y\",\n",
    "\"TwoBody_ENDVERTEX_Z\",\n",
    "\"TwoBody_OWNPV_X\",\n",
    "\"TwoBody_OWNPV_Y\",\n",
    "\"TwoBody_OWNPV_Z\"]\n",
    "\n",
    "extra_COM_stuff = ['TwoBody_Extra_Px',\n",
    "'TwoBody_Extra_Py',\n",
    "'TwoBody_Extra_Pz',\n",
    "'TwoBody_Extra_TRUEPID',\n",
    "'TwoBody_Extra_FromSameB',\n",
    "'TwoBody_Extra_CHARGE',\n",
    "'TwoBody_Extra_NNp',\n",
    " 'TwoBody_Extra_NNk',\n",
    " 'TwoBody_Extra_NNpi',\n",
    " 'TwoBody_Extra_NNmu',\n",
    " 'TwoBody_Extra_NNe',\n",
    "'TwoBody_Extra_NNg']\n",
    "\n",
    "EXTRAcols = ids + COM_stuff + extra_COM_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_root('TaggingJpsiK2012_tiny_fix_fix.root', columns=EXTRAcols, flatten = extra_COM_stuff )\n",
    "\n",
    "#as eventNumber is not unique, but for a given run number, it is, so we can combine the two to get a completely unique id for each event\n",
    "df.index = df.apply(lambda x:str(int(x.runNumber)) + str(int(x.eventNumber))+'-'+str(int(x.nCandidate)), axis=1 )\n",
    "\n",
    "#cutting out extra track canidates from TwoBodyCandidates that weren't recoginised by first stage MVA\n",
    "extra_tracks_df = df.loc[promising_TB_ids, :]\n",
    "\n",
    "#giving new_df a new index for each unique extra track\n",
    "extra_tracks_df.index = extra_tracks_df.apply(lambda x:str(int(x.runNumber)) + str(int(x.eventNumber))+'-'+str(int(x.nCandidate))+'-'+str(int(x.__array_index)), axis=1 )\n",
    "\n",
    "#cutting out the bad extra tracks\n",
    "promising_extra_tracks_ids = pd.read_csv('THIRD_round_extra_track_candidates_tiny.csv', header=None)[0]\n",
    "extra_tracks_df = extra_tracks_df.loc[promising_extra_tracks_ids, :]\n",
    "\n",
    "#get rid of other id like columns as now no need for them\n",
    "extra_tracks_df = extra_tracks_df.drop(columns=['runNumber', 'eventNumber', 'nCandidate', '__array_index'], axis=0)\n",
    "\n",
    "#calculating the COM variables for the extra tracks\n",
    "extra_tracks_COM_df = TOF_COM_calculator(df = extra_tracks_df, PXs=[\"TwoBody_Extra_Px\"], PYs=[\"TwoBody_Extra_Py\"], PZs=[\"TwoBody_Extra_Pz\"], proton_probs=['TwoBody_Extra_NNp'], kaon_probs=['TwoBody_Extra_NNk'], pion_probs=['TwoBody_Extra_NNpi'], electron_probs=['TwoBody_Extra_NNe'], muon_probs=['TwoBody_Extra_NNmu'], names=['extra_track'] )\n",
    "\n",
    "#deleting data objects which we no longer need to save RAM space\n",
    "del df, extra_tracks_df, promising_TB_ids, promising_extra_tracks_ids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add extra track COM info to the TB_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2467\n",
      "1203\n",
      "585\n",
      "296\n",
      "142\n",
      "71\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "#I need to add a TB_index column to extra tracks df so I can merge them later\n",
    "extra_tracks_COM_df['TB_id'] = extra_tracks_COM_df.apply(lambda x: x.name[:-2], axis=1 )\n",
    "\n",
    "count = 0\n",
    "#defing the extra tracks that need to be added, and looping over until theres none left\n",
    "need_adding = extra_tracks_COM_df\n",
    "while len(need_adding) != 0:\n",
    "   \n",
    "    print(len(need_adding))\n",
    "    \n",
    "    #defining the track ids that will be added to the TB_df next\n",
    "    being_added = need_adding['TB_id'].drop_duplicates().index.to_list()\n",
    "    #we also need to remove these ids from the tracks that need_adding list\n",
    "    updated_need_adding_ids = [track for track in need_adding.index if track not in being_added]\n",
    "    need_adding = need_adding.loc[updated_need_adding_ids]\n",
    "    \n",
    "    #initialising df to be merged with COM_TB_df, also getting rid of pointless features such as those already in TB df\n",
    "    feats = extra_COM_stuff + ['Eextra_track', 'TB_id']\n",
    "    being_added_df = extra_tracks_COM_df.loc[being_added, feats]\n",
    "    \n",
    "    #need to change index of being_added_df so it can be merged with the TB_df\n",
    "    being_added_df.index = being_added_df['TB_id']\n",
    "    being_added_df = being_added_df.drop(columns=(['TB_id', 'TwoBody_Extra_TRUEPID','TwoBody_Extra_FromSameB']))\n",
    "    #we also need to change names of the being_added_df columns as we will be adding more than one\n",
    "    being_added_df.columns = [name +'_' + str(count) for name in being_added_df.columns ]\n",
    "    #time to merge the being_added_df to the TB_df\n",
    "    COM_TB_df = pd.concat([COM_TB_df, being_added_df], axis=1)\n",
    "    \n",
    "    count+= 1\n",
    "    \n",
    "    \n",
    "    \n",
    "#because the way the concat works, we need to fill in the NaN values, lets try to replace them all with -1\n",
    "COM_TB_df = COM_TB_df.fillna(-1)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_features = ['pv_vector',\n",
    " 'sv_vector',\n",
    " 'flight',\n",
    "'boost_est',\n",
    " 'track1_p4',\n",
    " 'track1_p4_boosted',\n",
    " 'track2_p4',\n",
    " 'track2_p4_boosted',\n",
    "'p4B',\n",
    "                'p4B_est']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:07<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "feats =  [c for c in COM_TB_df.columns if c not in vector_features] \n",
    "X = COM_TB_df[feats]\n",
    "y = COM_TB_df['SignalB_ID']\n",
    "#make y binary\n",
    "y = y.replace(521, 1) ; y = y.replace(-521, 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=43)\n",
    "ids1 = X_train.index ; ids2 = X_test.index\n",
    "\n",
    "oof = pd.Series(np.zeros(y_train.shape[0]), index= ids1)\n",
    "preds = pd.Series(np.zeros(y_test.shape[0]), index= ids2)\n",
    "\n",
    "all_data = pd.concat([X_train, X_test])\n",
    "norm_data = StandardScaler().fit_transform(all_data)\n",
    "X_train = norm_data[:X_train.shape[0]] ; y_train = y_train.to_numpy().ravel()\n",
    "X_test = norm_data[X_train.shape[0]:] ; y_test = y_test.to_numpy().ravel()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "\n",
    "for train_idx, cv_idx in tqdm(skf.split(X_train, y_train), total=skf.n_splits):\n",
    "    model = LGBMClassifier()\n",
    "    model.fit(X_train[train_idx], y_train[train_idx])\n",
    "    oof.iloc[cv_idx] = model.predict_proba(X_train[cv_idx])[:,1]\n",
    "    preds.loc[ids2] += model.predict_proba(X_test)[:,1] / skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, round(oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, round(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-92cc231ef12a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/envs/cern_env/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    353\u001b[0m     return _average_binary_score(\n\u001b[1;32m    354\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cern_env/lib/python3.7/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "roc_auc_score(y_train, oof), precision_score(y_train, round(oof)) , recall_score(y_train, round(oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data=model.feature_importances_, index= X.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1., -1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-521.,  521.,   -1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COM_TB_df['SignalB_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
